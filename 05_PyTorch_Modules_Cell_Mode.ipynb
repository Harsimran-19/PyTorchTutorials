{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["_cCRxln4Odd8","sOEE-SdeN-ax","-G6V4gphN-g5","HQEaqbolN-kB","avC6deebN-m_","gEq5ZRlpN-py"],"authorship_tag":"ABX9TyMUHGGDfE/KRHgfLJvjLDs9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0b3b68487b7a4115bf3606b934213b28":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0ae28dca41324b6eb5d5cb9f7e75aa60","IPY_MODEL_ee0fa552e2ff4a6d80536db39f090493","IPY_MODEL_489ef4ba5c124261bcefcc3c370c26d6"],"layout":"IPY_MODEL_aca26dcbb41143d69ef311273f8bb1c8"}},"0ae28dca41324b6eb5d5cb9f7e75aa60":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_00079199028d45dba405dc4084c92b07","placeholder":"​","style":"IPY_MODEL_f396aebc5aa1444f9cb620ccccdbf773","value":"100%"}},"ee0fa552e2ff4a6d80536db39f090493":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_46d579656b074bd9bedd5ccfd96b2b88","max":5,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e1437b9f79d8441fa96bb26df29118f1","value":5}},"489ef4ba5c124261bcefcc3c370c26d6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0580c4f0203142ab83adfb1888290e53","placeholder":"​","style":"IPY_MODEL_e89b6b2b305842549d01b6612bd46729","value":" 5/5 [00:11&lt;00:00,  2.13s/it]"}},"aca26dcbb41143d69ef311273f8bb1c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"00079199028d45dba405dc4084c92b07":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f396aebc5aa1444f9cb620ccccdbf773":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"46d579656b074bd9bedd5ccfd96b2b88":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e1437b9f79d8441fa96bb26df29118f1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0580c4f0203142ab83adfb1888290e53":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e89b6b2b305842549d01b6612bd46729":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# Importing Dependices"],"metadata":{"id":"_cCRxln4Odd8"}},{"cell_type":"code","execution_count":12,"metadata":{"id":"nQoMBIjxN5hD","executionInfo":{"status":"ok","timestamp":1679672020640,"user_tz":-330,"elapsed":5,"user":{"displayName":"Harsimran Singh","userId":"10328925209778416644"}}},"outputs":[],"source":["import torch\n","from torch import nn\n","import matplotlib.pyplot as plt\n","from torchvision import transforms,datasets\n","from torch.utils.data import DataLoader\n","from torch.optim import Adam"]},{"cell_type":"markdown","source":["# Get Data"],"metadata":{"id":"sOEE-SdeN-ax"}},{"cell_type":"code","source":["%%writefile FoodVision_Modules/get_data.py\n","import os\n","import zipfile\n","\n","from pathlib import Path\n","\n","import requests\n","\n","# Setup path to data folder\n","data_path = Path(\"data/\")\n","image_path = data_path / \"pizza_steak_sushi\"\n","\n","# If the image folder doesn't exist, download it and prepare it... \n","if image_path.is_dir():\n","    print(f\"{image_path} directory exists.\")\n","else:\n","    print(f\"Did not find {image_path} directory, creating one...\")\n","    image_path.mkdir(parents=True, exist_ok=True)\n","    \n","# Download pizza, steak, sushi data\n","with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n","    request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n","    print(\"Downloading pizza, steak, sushi data...\")\n","    f.write(request.content)\n","\n","# Unzip pizza, steak, sushi data\n","with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n","    print(\"Unzipping pizza, steak, sushi data...\") \n","    zip_ref.extractall(image_path)\n","    \n","# Remove zip file\n","os.remove(data_path / \"pizza_steak_sushi.zip\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OFSN1vcwOg-5","executionInfo":{"status":"ok","timestamp":1679676197608,"user_tz":-330,"elapsed":410,"user":{"displayName":"Harsimran Singh","userId":"10328925209778416644"}},"outputId":"fec88dd4-8b31-4be6-e0bf-49058653de5d"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing FoodVision_Modules/get_data.py\n"]}]},{"cell_type":"code","source":["# Setup train and testing paths\n","train_dir = image_path / \"train\"\n","test_dir = image_path / \"test\"\n","\n","train_dir, test_dir"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JoThWmtAO9SV","executionInfo":{"status":"ok","timestamp":1679671695854,"user_tz":-330,"elapsed":4,"user":{"displayName":"Harsimran Singh","userId":"10328925209778416644"}},"outputId":"b570beb7-ed52-4a1d-90c2-5957ddf126ec"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(PosixPath('data/pizza_steak_sushi/train'),\n"," PosixPath('data/pizza_steak_sushi/test'))"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["# Creating DataLoaders"],"metadata":{"id":"-G6V4gphN-g5"}},{"cell_type":"code","source":["BATCH_SIZE=32"],"metadata":{"id":"2RWzI5KtQIEv","executionInfo":{"status":"ok","timestamp":1679671992803,"user_tz":-330,"elapsed":5,"user":{"displayName":"Harsimran Singh","userId":"10328925209778416644"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["transform=transforms.Compose([\n","    transforms.Resize((64,64)),\n","    transforms.ToTensor()\n","])"],"metadata":{"id":"Z1WmEI_pOheL","executionInfo":{"status":"ok","timestamp":1679671809458,"user_tz":-330,"elapsed":1495,"user":{"displayName":"Harsimran Singh","userId":"10328925209778416644"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["train_data=datasets.ImageFolder(\n","    root=train_dir,\n","    transform=transform,\n","    target_transform=None\n",")"],"metadata":{"id":"yNpgvlrLPOI9","executionInfo":{"status":"ok","timestamp":1679671903311,"user_tz":-330,"elapsed":5,"user":{"displayName":"Harsimran Singh","userId":"10328925209778416644"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["test_data=datasets.ImageFolder(\n","    root=test_dir,\n","    transform=transform,\n",")"],"metadata":{"id":"FD_rM5hrPOL3","executionInfo":{"status":"ok","timestamp":1679671906413,"user_tz":-330,"elapsed":7,"user":{"displayName":"Harsimran Singh","userId":"10328925209778416644"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["class_names=train_data.classes"],"metadata":{"id":"0fLttDkHQUOq","executionInfo":{"status":"ok","timestamp":1679672051578,"user_tz":-330,"elapsed":5,"user":{"displayName":"Harsimran Singh","userId":"10328925209778416644"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["train_dataLoader=DataLoader(\n","    train_data,\n","    batch_size=BATCH_SIZE,\n","    num_workers=1,\n","    shuffle=True\n",")"],"metadata":{"id":"_W6uIlnUPOOe","executionInfo":{"status":"ok","timestamp":1679672087324,"user_tz":-330,"elapsed":532,"user":{"displayName":"Harsimran Singh","userId":"10328925209778416644"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["test_dataLoader=DataLoader(\n","    test_data,\n","    batch_size=BATCH_SIZE,\n","    num_workers=1,\n","    shuffle=False\n",")"],"metadata":{"id":"fQqJqPpNQB1u","executionInfo":{"status":"ok","timestamp":1679672087325,"user_tz":-330,"elapsed":6,"user":{"displayName":"Harsimran Singh","userId":"10328925209778416644"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["%%writefile FoodVision_Modules/data_setup.py\n","import os\n","\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","\n","NUM_WORKERS = os.cpu_count()\n","\n","def create_dataloaders(\n","    train_dir: str, \n","    test_dir: str, \n","    transform: transforms.Compose, \n","    batch_size: int, \n","    num_workers: int=NUM_WORKERS\n","):\n","# Use ImageFolder to create dataset(s)\n","  train_data = datasets.ImageFolder(train_dir, transform=transform)\n","  test_data = datasets.ImageFolder(test_dir, transform=transform)\n","\n","  # Get class names\n","  class_names = train_data.classes\n","\n","  # Turn images into data loaders\n","  train_dataloader = DataLoader(\n","      train_data,\n","      batch_size=batch_size,\n","      shuffle=True,\n","      num_workers=num_workers,\n","      pin_memory=True,\n","  )\n","  test_dataloader = DataLoader(\n","      test_data,\n","      batch_size=batch_size,\n","      shuffle=True,\n","      num_workers=num_workers,\n","      pin_memory=True,\n","  )\n","\n","  return train_dataloader, test_dataloader, class_names"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WjI7CWHZgQCl","executionInfo":{"status":"ok","timestamp":1679676355202,"user_tz":-330,"elapsed":5,"user":{"displayName":"Harsimran Singh","userId":"10328925209778416644"}},"outputId":"e17a7077-a305-4ccb-bad3-f091b2912c33"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing FoodVision_Modules/data_setup.py\n"]}]},{"cell_type":"markdown","source":["# Creating the Model"],"metadata":{"id":"HQEaqbolN-kB"}},{"cell_type":"code","source":["%%writefile FoodVision_Modules/model_builder.py\n","import torch \n","from torch import nn\n","class TinyVGG(nn.Module):\n","  def __init__(self,input_shape: int, hidden_units: int, output_shape: int):\n","    super().__init__()\n","    self.conv_block_1 = nn.Sequential(\n","          nn.Conv2d(in_channels=input_shape, \n","                    out_channels=hidden_units, \n","                    kernel_size=3, # how big is the square that's going over the image?\n","                    stride=1, # default\n","                    padding=0), # options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number \n","          nn.ReLU(),\n","          nn.Conv2d(in_channels=hidden_units, \n","                    out_channels=hidden_units,\n","                    kernel_size=3,\n","                    stride=1,\n","                    padding=0),\n","          nn.ReLU(),\n","          nn.MaxPool2d(kernel_size=2,\n","                        stride=2) # default stride value is same as kernel_size\n","      )\n","    self.conv_block_2 = nn.Sequential(\n","        nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n","        nn.ReLU(),\n","        nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n","        nn.ReLU(),\n","        nn.MaxPool2d(2)\n","    )\n","    self.classifier = nn.Sequential(\n","        nn.Flatten(),\n","        # Where did this in_features shape come from? \n","        # It's because each layer of our network compresses and changes the shape of our inputs data.\n","        nn.Linear(in_features=hidden_units*13*13,\n","                  out_features=output_shape)\n","    )\n","  def forward(self,x):\n","    x = self.conv_block_1(x)\n","    x = self.conv_block_2(x)\n","    x = self.classifier(x)\n","    return x"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WbK-8S6kOh5D","executionInfo":{"status":"ok","timestamp":1679676448787,"user_tz":-330,"elapsed":801,"user":{"displayName":"Harsimran Singh","userId":"10328925209778416644"}},"outputId":"a540d3f8-b784-47e8-aad4-22a390bc4c9c"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing FoodVision_Modules/model_builder.py\n"]}]},{"cell_type":"code","source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\""],"metadata":{"id":"mLc56662RNLQ","executionInfo":{"status":"ok","timestamp":1679672272336,"user_tz":-330,"elapsed":8,"user":{"displayName":"Harsimran Singh","userId":"10328925209778416644"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(42)\n","model_0 = TinyVGG(input_shape=3, # number of color channels (3 for RGB) \n","                  hidden_units=10, \n","                  output_shape=len(train_data.classes)).to(device)\n","model_0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RSFEApZ8RRJ1","executionInfo":{"status":"ok","timestamp":1679672288107,"user_tz":-330,"elapsed":11,"user":{"displayName":"Harsimran Singh","userId":"10328925209778416644"}},"outputId":"e1e678a1-8757-4c08-b57a-62c2a49d7226"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TinyVGG(\n","  (conv_block_1): Sequential(\n","    (0): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1))\n","    (1): ReLU()\n","    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n","    (3): ReLU()\n","    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (conv_block_2): Sequential(\n","    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n","    (1): ReLU()\n","    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n","    (3): ReLU()\n","    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (classifier): Sequential(\n","    (0): Flatten(start_dim=1, end_dim=-1)\n","    (1): Linear(in_features=1690, out_features=3, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["# Train Functions"],"metadata":{"id":"avC6deebN-m_"}},{"cell_type":"code","source":["def train_step(model: torch.nn.Module, \n","               dataloader: torch.utils.data.DataLoader, \n","               loss_fn: torch.nn.Module, \n","               optimizer: torch.optim.Optimizer,\n","               device: torch.device):\n","  model.train()\n","  train_loss,train_acc=0,0\n","  for batch,(X,y) in enumerate(dataloader):\n","    # Send data to target device\n","      X, y = X.to(device), y.to(device)\n","      # 1. Forward pass\n","      y_pred = model(X)\n","      # 2. Calculate  and accumulate loss\n","      loss = loss_fn(y_pred, y)\n","      train_loss += loss.item() \n","      # 3. Optimizer zero grad\n","      optimizer.zero_grad()\n","      # 4. Loss backward\n","      loss.backward()\n","      # 5. Optimizer step\n","      optimizer.step()\n","      # Calculate and accumulate accuracy metric across all batches\n","      y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n","      train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n","  # Adjust metrics to get average loss and accuracy per batch \n","  train_loss = train_loss / len(dataloader)\n","  train_acc = train_acc / len(dataloader)\n","  return train_loss, train_acc    "],"metadata":{"id":"c0wqXu9rOiXh","executionInfo":{"status":"ok","timestamp":1679675436817,"user_tz":-330,"elapsed":449,"user":{"displayName":"Harsimran Singh","userId":"10328925209778416644"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["def test_step(model: torch.nn.Module, \n","              dataloader: torch.utils.data.DataLoader, \n","              loss_fn: torch.nn.Module,\n","              device: torch.device):\n","  model.eval()\n","  test_loss, test_acc = 0, 0\n","  with torch.inference_mode():\n","    for batch,(X,y) in enumerate(dataloader):\n","      # Send data to target device\n","      X, y = X.to(device), y.to(device)\n","  \n","      # 1. Forward pass\n","      test_pred_logits = model(X)\n","      # 2. Calculate and accumulate loss\n","      loss = loss_fn(test_pred_logits, y)\n","      test_loss += loss.item()\n","      \n","      # Calculate and accumulate accuracy\n","      test_pred_labels = test_pred_logits.argmax(dim=1)\n","      test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n","    # Adjust metrics to get average loss and accuracy per batch \n","    test_loss = test_loss / len(dataloader)\n","    test_acc = test_acc / len(dataloader)\n","    return test_loss, test_acc  "],"metadata":{"id":"JeDe4tLQRfFd","executionInfo":{"status":"ok","timestamp":1679675195400,"user_tz":-330,"elapsed":399,"user":{"displayName":"Harsimran Singh","userId":"10328925209778416644"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["from tqdm.auto import tqdm \n","def train(model: torch.nn.Module, \n","          train_dataloader: torch.utils.data.DataLoader, \n","          test_dataloader: torch.utils.data.DataLoader, \n","          optimizer: torch.optim.Optimizer,\n","          loss_fn: torch.nn.Module,\n","          epochs: int,\n","          device: torch.device):\n","  results = {\"train_loss\": [],\n","      \"train_acc\": [],\n","      \"test_loss\": [],\n","      \"test_acc\": []\n","  }\n","  for epoch in tqdm(range(epochs)):\n","      train_loss, train_acc = train_step(model=model,\n","                                          dataloader=train_dataloader,\n","                                          loss_fn=loss_fn,\n","                                          optimizer=optimizer,\n","                                          device=device)\n","      test_loss, test_acc = test_step(model=model,\n","          dataloader=test_dataloader,\n","          loss_fn=loss_fn,\n","          device=device)\n","      \n","      # Print out what's happening\n","      print(\n","          f\"Epoch: {epoch+1} | \"\n","          f\"train_loss: {train_loss:.4f} | \"\n","          f\"train_acc: {train_acc:.4f} | \"\n","          f\"test_loss: {test_loss:.4f} | \"\n","          f\"test_acc: {test_acc:.4f}\"\n","      )\n","\n","      # Update results dictionary\n","      results[\"train_loss\"].append(train_loss)\n","      results[\"train_acc\"].append(train_acc)\n","      results[\"test_loss\"].append(test_loss)\n","      results[\"test_acc\"].append(test_acc)\n","\n","  # Return the filled results at the end of the epochs\n","  return results"],"metadata":{"id":"QBm28OSDRfJX","executionInfo":{"status":"ok","timestamp":1679675302748,"user_tz":-330,"elapsed":611,"user":{"displayName":"Harsimran Singh","userId":"10328925209778416644"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["%%writefile FoodVision_Modules/engine.py\n","import torch\n","from tqdm.auto import tqdm\n","def train_step(model: torch.nn.Module, \n","               dataloader: torch.utils.data.DataLoader, \n","               loss_fn: torch.nn.Module, \n","               optimizer: torch.optim.Optimizer,\n","               device: torch.device):\n","  model.train()\n","  train_loss,train_acc=0,0\n","  for batch,(X,y) in enumerate(dataloader):\n","    # Send data to target device\n","      X, y = X.to(device), y.to(device)\n","      # 1. Forward pass\n","      y_pred = model(X)\n","      # 2. Calculate  and accumulate loss\n","      loss = loss_fn(y_pred, y)\n","      train_loss += loss.item() \n","      # 3. Optimizer zero grad\n","      optimizer.zero_grad()\n","      # 4. Loss backward\n","      loss.backward()\n","      # 5. Optimizer step\n","      optimizer.step()\n","      # Calculate and accumulate accuracy metric across all batches\n","      y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n","      train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n","  # Adjust metrics to get average loss and accuracy per batch \n","  train_loss = train_loss / len(dataloader)\n","  train_acc = train_acc / len(dataloader)\n","  return train_loss, train_acc  \n","def test_step(model: torch.nn.Module, \n","              dataloader: torch.utils.data.DataLoader, \n","              loss_fn: torch.nn.Module,\n","              device: torch.device):\n","  model.eval()\n","  test_loss, test_acc = 0, 0\n","  with torch.inference_mode():\n","    for batch,(X,y) in enumerate(dataloader):\n","      # Send data to target device\n","      X, y = X.to(device), y.to(device)\n","  \n","      # 1. Forward pass\n","      test_pred_logits = model(X)\n","      # 2. Calculate and accumulate loss\n","      loss = loss_fn(test_pred_logits, y)\n","      test_loss += loss.item()\n","      \n","      # Calculate and accumulate accuracy\n","      test_pred_labels = test_pred_logits.argmax(dim=1)\n","      test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n","    # Adjust metrics to get average loss and accuracy per batch \n","    test_loss = test_loss / len(dataloader)\n","    test_acc = test_acc / len(dataloader)\n","    return test_loss, test_acc  \n","def train(model: torch.nn.Module, \n","          train_dataloader: torch.utils.data.DataLoader, \n","          test_dataloader: torch.utils.data.DataLoader, \n","          optimizer: torch.optim.Optimizer,\n","          loss_fn: torch.nn.Module,\n","          epochs: int,\n","          device: torch.device):\n","  results = {\"train_loss\": [],\n","      \"train_acc\": [],\n","      \"test_loss\": [],\n","      \"test_acc\": []\n","  }\n","  for epoch in tqdm(range(epochs)):\n","      train_loss, train_acc = train_step(model=model,\n","                                          dataloader=train_dataloader,\n","                                          loss_fn=loss_fn,\n","                                          optimizer=optimizer,\n","                                          device=device)\n","      test_loss, test_acc = test_step(model=model,\n","          dataloader=test_dataloader,\n","          loss_fn=loss_fn,\n","          device=device)\n","      \n","      # Print out what's happening\n","      print(\n","          f\"Epoch: {epoch+1} | \"\n","          f\"train_loss: {train_loss:.4f} | \"\n","          f\"train_acc: {train_acc:.4f} | \"\n","          f\"test_loss: {test_loss:.4f} | \"\n","          f\"test_acc: {test_acc:.4f}\"\n","      )\n","\n","      # Update results dictionary\n","      results[\"train_loss\"].append(train_loss)\n","      results[\"train_acc\"].append(train_acc)\n","      results[\"test_loss\"].append(test_loss)\n","      results[\"test_acc\"].append(test_acc)\n","\n","  # Return the filled results at the end of the epochs\n","  return results"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"chMab1ithPOx","executionInfo":{"status":"ok","timestamp":1679676613979,"user_tz":-330,"elapsed":537,"user":{"displayName":"Harsimran Singh","userId":"10328925209778416644"}},"outputId":"f9e760e4-12d2-41df-c1eb-2da6e690fbcf"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing FoodVision_Modules/engine.py\n"]}]},{"cell_type":"markdown","source":["# Save Functions"],"metadata":{"id":"gEq5ZRlpN-py"}},{"cell_type":"code","source":["%%writefile FoodVision_Modules/utils.py\n","import torch\n","from pathlib import Path\n","\n","def save_model(model: torch.nn.Module,\n","               target_dir: str,\n","               model_name: str):\n","  \"\"\"Saves a PyTorch model to a target directory.\n","\n","  Args:\n","    model: A target PyTorch model to save.\n","    target_dir: A directory for saving the model to.\n","    model_name: A filename for the saved model. Should include\n","      either \".pth\" or \".pt\" as the file extension.\n","  \n","  Example usage:\n","    save_model(model=model_0,\n","               target_dir=\"models\",\n","               model_name=\"05_going_modular_tingvgg_model.pth\")\n","  \"\"\"\n","  # Create target directory\n","  target_dir_path = Path(target_dir)\n","  target_dir_path.mkdir(parents=True,\n","                        exist_ok=True)\n","  \n","  # Create model save path\n","  assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"), \"model_name should end with '.pt' or '.pth'\"\n","  model_save_path = target_dir_path / model_name\n","\n","  # Save the model state_dict()\n","  print(f\"[INFO] Saving model to: {model_save_path}\")\n","  torch.save(obj=model.state_dict(),\n","             f=model_save_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IoI2s0qnOixh","executionInfo":{"status":"ok","timestamp":1679676665670,"user_tz":-330,"elapsed":411,"user":{"displayName":"Harsimran Singh","userId":"10328925209778416644"}},"outputId":"da411598-20d9-4edc-84ef-b4aea5064a5b"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing FoodVision_Modules/utils.py\n"]}]},{"cell_type":"markdown","source":["# Training and Saving the Model"],"metadata":{"id":"fd7D2QS7N-s5"}},{"cell_type":"code","source":["# Set number of epochs\n","NUM_EPOCHS = 5"],"metadata":{"id":"ImgQtg98N9vp","executionInfo":{"status":"ok","timestamp":1679675336356,"user_tz":-330,"elapsed":512,"user":{"displayName":"Harsimran Singh","userId":"10328925209778416644"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["# Recreate an instance of TinyVGG\n","model_0 = TinyVGG(input_shape=3, # number of color channels (3 for RGB) \n","                  hidden_units=10, \n","                  output_shape=len(train_data.classes)).to(device)"],"metadata":{"id":"KgSzpF3MN9yv","executionInfo":{"status":"ok","timestamp":1679675344377,"user_tz":-330,"elapsed":450,"user":{"displayName":"Harsimran Singh","userId":"10328925209778416644"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["# Setup loss function and optimizer\n","loss_fn = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(params=model_0.parameters(), lr=0.001)"],"metadata":{"id":"Sr3HNgfrN91p","executionInfo":{"status":"ok","timestamp":1679675350503,"user_tz":-330,"elapsed":1583,"user":{"displayName":"Harsimran Singh","userId":"10328925209778416644"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["# Start the timer\n","from timeit import default_timer as timer \n","start_time = timer()\n","\n","# Train model_0 \n","model_0_results = train(model=model_0, \n","                        train_dataloader=train_dataLoader,\n","                        test_dataloader=test_dataLoader,\n","                        optimizer=optimizer,\n","                        loss_fn=loss_fn, \n","                        epochs=NUM_EPOCHS,\n","                        device=device)\n","\n","# End the timer and print out how long it took\n","end_time = timer()\n","print(f\"[INFO] Total training time: {end_time-start_time:.3f} seconds\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":158,"referenced_widgets":["0b3b68487b7a4115bf3606b934213b28","0ae28dca41324b6eb5d5cb9f7e75aa60","ee0fa552e2ff4a6d80536db39f090493","489ef4ba5c124261bcefcc3c370c26d6","aca26dcbb41143d69ef311273f8bb1c8","00079199028d45dba405dc4084c92b07","f396aebc5aa1444f9cb620ccccdbf773","46d579656b074bd9bedd5ccfd96b2b88","e1437b9f79d8441fa96bb26df29118f1","0580c4f0203142ab83adfb1888290e53","e89b6b2b305842549d01b6612bd46729"]},"id":"_R8u8gs4N94f","executionInfo":{"status":"ok","timestamp":1679675455176,"user_tz":-330,"elapsed":13106,"user":{"displayName":"Harsimran Singh","userId":"10328925209778416644"}},"outputId":"33d139a5-31b6-4bd7-d4d5-3326382f191f"},"execution_count":29,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/5 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b3b68487b7a4115bf3606b934213b28"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch: 1 | train_loss: 1.1027 | train_acc: 0.2617 | test_loss: 1.0900 | test_acc: 0.2604\n","Epoch: 2 | train_loss: 1.0967 | train_acc: 0.2812 | test_loss: 1.0806 | test_acc: 0.5417\n","Epoch: 3 | train_loss: 1.0865 | train_acc: 0.4648 | test_loss: 1.0839 | test_acc: 0.2604\n","Epoch: 4 | train_loss: 1.0988 | train_acc: 0.3047 | test_loss: 1.0989 | test_acc: 0.2812\n","Epoch: 5 | train_loss: 1.0816 | train_acc: 0.3672 | test_loss: 1.1089 | test_acc: 0.3021\n","[INFO] Total training time: 11.568 seconds\n"]}]},{"cell_type":"code","source":["# Save the model\n","save_model(model=model_0,\n","           target_dir=\"models\",\n","           model_name=\"05_going_modular_cell_mode_tinyvgg_model.pth\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xUcxkw0EdIci","executionInfo":{"status":"ok","timestamp":1679675459412,"user_tz":-330,"elapsed":1085,"user":{"displayName":"Harsimran Singh","userId":"10328925209778416644"}},"outputId":"ad3df5be-4d24-4a16-a830-0db0b24d3ba6"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO] Saving model to: models/05_going_modular_cell_mode_tinyvgg_model.pth\n"]}]},{"cell_type":"code","source":["%%writefile FoodVision_Modules/train.py\n","import os\n","import torch\n","import data_setup, engine, model_builder, utils\n","\n","from torchvision import transforms\n","# Setup hyperparameters\n","NUM_EPOCHS = 5\n","BATCH_SIZE = 32\n","HIDDEN_UNITS = 10\n","LEARNING_RATE = 0.001\n","# Setup directories\n","train_dir = \"data/pizza_steak_sushi/train\"\n","test_dir = \"data/pizza_steak_sushi/test\"\n","\n","# Setup target device\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# Create transforms\n","data_transform = transforms.Compose([\n","  transforms.Resize((64, 64)),\n","  transforms.ToTensor()\n","])\n","# Create DataLoaders with help from data_setup.py\n","train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n","    train_dir=train_dir,\n","    test_dir=test_dir,\n","    transform=data_transform,\n","    batch_size=BATCH_SIZE\n",")\n","\n","# Create model with help from model_builder.py\n","model = model_builder.TinyVGG(\n","    input_shape=3,\n","    hidden_units=HIDDEN_UNITS,\n","    output_shape=len(class_names)\n",").to(device)\n","\n","# Set loss and optimizer\n","loss_fn = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(),\n","                             lr=LEARNING_RATE)\n","\n","# Start training with help from engine.py\n","engine.train(model=model,\n","             train_dataloader=train_dataloader,\n","             test_dataloader=test_dataloader,\n","             loss_fn=loss_fn,\n","             optimizer=optimizer,\n","             epochs=NUM_EPOCHS,\n","             device=device)\n","\n","# Save the model with help from utils.py\n","utils.save_model(model=model,\n","                 target_dir=\"models\",\n","                 model_name=\"05_going_modular_script_mode_tinyvgg_model.pth\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IzNRai4KiEp3","executionInfo":{"status":"ok","timestamp":1679676764791,"user_tz":-330,"elapsed":922,"user":{"displayName":"Harsimran Singh","userId":"10328925209778416644"}},"outputId":"2c0ecab7-83b4-4297-bcd1-6dc3b9a4d62b"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing FoodVision_Modules/train.py\n"]}]},{"cell_type":"code","source":["!zip -r /content/FoodVision_Modules.zip /content/FoodVision_Modules"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ufJKgkMSjjGR","executionInfo":{"status":"ok","timestamp":1679677109985,"user_tz":-330,"elapsed":12,"user":{"displayName":"Harsimran Singh","userId":"10328925209778416644"}},"outputId":"11d7336e-5ba3-4a88-b960-861b86303030"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["  adding: content/FoodVision_Modules/ (stored 0%)\n","  adding: content/FoodVision_Modules/utils.py (deflated 57%)\n","  adding: content/FoodVision_Modules/engine.py (deflated 73%)\n","  adding: content/FoodVision_Modules/data_setup.py (deflated 63%)\n","  adding: content/FoodVision_Modules/model_builder.py (deflated 65%)\n","  adding: content/FoodVision_Modules/train.py (deflated 57%)\n","  adding: content/FoodVision_Modules/get_data.py (deflated 55%)\n"]}]},{"cell_type":"code","source":["from google.colab import files\n","files.download('/content/FoodVision_Modules.zip')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"QRU5nTTEj0if","executionInfo":{"status":"ok","timestamp":1679677221012,"user_tz":-330,"elapsed":441,"user":{"displayName":"Harsimran Singh","userId":"10328925209778416644"}},"outputId":"cfa1d260-9ea9-4985-dd8a-b9d3deeefe03"},"execution_count":44,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_0fe23d5e-e842-4b20-b1c6-9a435b042e8f\", \"FoodVision_Modules.zip\", 4821)"]},"metadata":{}}]}]}