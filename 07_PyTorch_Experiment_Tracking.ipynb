{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNVV4Rv7cQf0KHw5rsOq9G2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# Importing Dependices"],"metadata":{"id":"s0FCRzrrsfCs"}},{"cell_type":"code","source":["!unzip Basic_Modules.zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"otszRruKpoNs","executionInfo":{"status":"ok","timestamp":1679752875819,"user_tz":-330,"elapsed":31,"user":{"displayName":"Harsimran Singh","userId":"10328925209778416644"}},"outputId":"bf293093-c23b-4da7-ca34-37186745e1fe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["unzip:  cannot find or open Basic_Modules.zip, Basic_Modules.zip.zip or Basic_Modules.zip.ZIP.\n"]}]},{"cell_type":"code","source":["!pip install torchinfo"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YwCr6dCQrqou","executionInfo":{"status":"ok","timestamp":1679752879915,"user_tz":-330,"elapsed":4112,"user":{"displayName":"Harsimran Singh","userId":"10328925209778416644"}},"outputId":"c7d09a6f-7666-4106-eea3-f16c5c544a02"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torchinfo\n","  Downloading torchinfo-1.7.2-py3-none-any.whl (22 kB)\n","Installing collected packages: torchinfo\n","Successfully installed torchinfo-1.7.2\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"786WA3xCpgtl","executionInfo":{"status":"error","timestamp":1679752884551,"user_tz":-330,"elapsed":4643,"user":{"displayName":"Harsimran Singh","userId":"10328925209778416644"}},"outputId":"d0d75bb5-471e-48ba-8fbc-bd51cd3f0e33","colab":{"base_uri":"https://localhost:8080/","height":352}},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-3ba5232930f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchinfo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mBasic_Modules\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_setup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Basic_Modules'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["import torch\n","import torchvision\n","from torch import nn\n","import matplotlib.pyplot as plt\n","from torchvision import transforms\n","from torchinfo import summary\n","from Basic_Modules import engine,data_setup"]},{"cell_type":"code","source":["device=\"cuda\" if torch.cuda.is_available() else \"cpu\""],"metadata":{"id":"JGKFdvghzlfd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device"],"metadata":{"id":"3OD4g2PHzvTj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["engine"],"metadata":{"id":"bZwCf3GtpoQk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Get Data"],"metadata":{"id":"ylqmWb-jsjqr"}},{"cell_type":"code","source":["import os\n","import zipfile\n","\n","from pathlib import Path\n","\n","import requests\n","\n","def download_data(source: str, \n","                  destination: str,\n","                  remove_source: bool = True) -> Path:\n","    \"\"\"Downloads a zipped dataset from source and unzips to destination.\n","\n","    Args:\n","        source (str): A link to a zipped file containing data.\n","        destination (str): A target directory to unzip data to.\n","        remove_source (bool): Whether to remove the source after downloading and extracting.\n","    \n","    Returns:\n","        pathlib.Path to downloaded data.\n","    \n","    Example usage:\n","        download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n","                      destination=\"pizza_steak_sushi\")\n","    \"\"\"\n","    # Setup path to data folder\n","    data_path = Path(\"data/\")\n","    image_path = data_path / destination\n","\n","    # If the image folder doesn't exist, download it and prepare it... \n","    if image_path.is_dir():\n","        print(f\"[INFO] {image_path} directory exists, skipping download.\")\n","    else:\n","        print(f\"[INFO] Did not find {image_path} directory, creating one...\")\n","        image_path.mkdir(parents=True, exist_ok=True)\n","        \n","        # Download pizza, steak, sushi data\n","        target_file = Path(source).name\n","        with open(data_path / target_file, \"wb\") as f:\n","            request = requests.get(source)\n","            print(f\"[INFO] Downloading {target_file} from {source}...\")\n","            f.write(request.content)\n","\n","        # Unzip pizza, steak, sushi data\n","        with zipfile.ZipFile(data_path / target_file, \"r\") as zip_ref:\n","            print(f\"[INFO] Unzipping {target_file} data...\") \n","            zip_ref.extractall(image_path)\n","\n","        # Remove .zip file\n","        if remove_source:\n","            os.remove(data_path / target_file)\n","    \n","    return image_path"],"metadata":{"id":"zxJxbFUopoTr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n","                           destination=\"pizza_steak_sushi\")\n","image_path"],"metadata":{"id":"wbKiCUEnpoWz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Create Dataset and DataLoaders"],"metadata":{"id":"EPuz_Z7rtS6M"}},{"cell_type":"code","source":["train_dir=image_path/\"train\"\n","test_dir=image_path/\"test\""],"metadata":{"id":"3Q-QSfJptWV8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Using Manual Transforms"],"metadata":{"id":"xOW8OMnlwkAL"}},{"cell_type":"code","source":["normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                                 std=[0.229, 0.224, 0.225])"],"metadata":{"id":"IFwhyw6ztWY6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["transform=transforms.Compose([\n","    transforms.ToTensor(),\n","    normalize,\n","    transforms.Resize((224,224))\n","])"],"metadata":{"id":"swvtSgR7tWb6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(transform)"],"metadata":{"id":"Iv-7sybptWfL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["BATCH_SIZE=32"],"metadata":{"id":"fGNM3jnjtWiN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataLoader,test_dataLoader,class_names=data_setup.create_dataloaders(\n","    batch_size=BATCH_SIZE,\n","    train_dir=train_dir,\n","    test_dir=test_dir,\n","    transform=transform\n",")"],"metadata":{"id":"LeQSbfqGtWlk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataLoader"],"metadata":{"id":"VCenFngnv1Fj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Using Automatic Transforms"],"metadata":{"id":"JwLuzAPRwpCL"}},{"cell_type":"code","source":["weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT"],"metadata":{"id":"uGZRGL3Tv2pb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["automatic_transforms = weights.transforms() \n","print(f\"Automatically created transforms: {automatic_transforms}\")"],"metadata":{"id":"3iNQhAdZv2sV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataLoader,test_dataLoader,class_names=data_setup.create_dataloaders(\n","    batch_size=BATCH_SIZE,\n","    train_dir=train_dir,\n","    test_dir=test_dir,\n","    transform=automatic_transforms\n",")"],"metadata":{"id":"apUFUSMvv2vb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Creating a Model"],"metadata":{"id":"tDfh6SljzcSh"}},{"cell_type":"code","source":["# Note: This is how a pretrained model would be created in torchvision > 0.13, it will be deprecated in future versions.\n","# model = torchvision.models.efficientnet_b0(pretrained=True).to(device) # OLD \n","\n","# Download the pretrained weights for EfficientNet_B0\n","weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # NEW in torchvision 0.13, \"DEFAULT\" means \"best weights available\"\n","\n","# Setup the model with the pretrained weights and send it to the target device\n","model = torchvision.models.efficientnet_b0(weights=weights).to(device)\n","\n","# View the output of the model\n","# model"],"metadata":{"id":"4H8J20nBv2ya"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Freeze all base layers by setting requires_grad attribute to False\n","for param in model.features.parameters():\n","    param.requires_grad = False\n","    \n","# Update the classifier head to suit our problem\n","model.classifier = torch.nn.Sequential(\n","    nn.Dropout(p=0.2, inplace=True),\n","    nn.Linear(in_features=1280, \n","              out_features=len(class_names),\n","              bias=True).to(device))"],"metadata":{"id":"M_Zv83aTv21z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["summary(model, \n","        input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\" (batch_size, color_channels, height, width)\n","        verbose=0,\n","        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n","        col_width=20,\n","        row_settings=[\"var_names\"]\n",")"],"metadata":{"id":"Em7N2J0yzip6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training the Model"],"metadata":{"id":"9xUKYL5iz-_y"}},{"cell_type":"code","source":["# Define loss and optimizer\n","loss_fn = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"],"metadata":{"id":"EcUw2c2Izis6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.tensorboard import SummaryWriter\n","\n","# Create a writer with all default settings\n","writer = SummaryWriter()"],"metadata":{"id":"RnmP2Q5bzivp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from Basic_Modules.engine import train_step,test_step\n","from tqdm.auto import tqdm"],"metadata":{"id":"rcu7MLCcziyq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(model: torch.nn.Module, \n","          train_dataloader: torch.utils.data.DataLoader, \n","          test_dataloader: torch.utils.data.DataLoader, \n","          optimizer: torch.optim.Optimizer,\n","          loss_fn: torch.nn.Module,\n","          epochs: int,\n","          device: torch.device,\n","          writer: torch.utils.tensorboard.writer.SummaryWriter):\n","  results = {\"train_loss\": [],\n","      \"train_acc\": [],\n","      \"test_loss\": [],\n","      \"test_acc\": []\n","  }\n","  for epoch in tqdm(range(epochs)):\n","      train_loss, train_acc = train_step(model=model,\n","                                          dataloader=train_dataloader,\n","                                          loss_fn=loss_fn,\n","                                          optimizer=optimizer,\n","                                          device=device)\n","      test_loss, test_acc = test_step(model=model,\n","          dataloader=test_dataloader,\n","          loss_fn=loss_fn,\n","          device=device)\n","      \n","      # Print out what's happening\n","      print(\n","          f\"Epoch: {epoch+1} | \"\n","          f\"train_loss: {train_loss:.4f} | \"\n","          f\"train_acc: {train_acc:.4f} | \"\n","          f\"test_loss: {test_loss:.4f} | \"\n","          f\"test_acc: {test_acc:.4f}\"\n","      )\n","\n","      # Update results dictionary\n","      results[\"train_loss\"].append(train_loss)\n","      results[\"train_acc\"].append(train_acc)\n","      results[\"test_loss\"].append(test_loss)\n","      results[\"test_acc\"].append(test_acc)\n","      if writer:\n","        writer.add_scalars(main_tag=\"Loss\", \n","                             tag_scalar_dict={\"train_loss\": train_loss,\n","                                              \"test_loss\": test_loss},\n","                             global_step=epoch)\n","        # Add accuracy results to SummaryWriter\n","        writer.add_scalars(main_tag=\"Accuracy\", \n","                             tag_scalar_dict={\"train_acc\": train_acc,\n","                                              \"test_acc\": test_acc}, \n","                             global_step=epoch)\n","        writer.add_graph(model=model, \n","                           # Pass in an example input\n","                           input_to_model=torch.randn(32, 3, 224, 224).to(device))\n","        writer.close()\n","      else:\n","        pass\n","\n","  # Return the filled results at the end of the epochs\n","  return results"],"metadata":{"id":"GtS4OGWEzi1Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results = train(model=model,\n","                train_dataloader=train_dataLoader,\n","                test_dataloader=test_dataLoader,\n","                optimizer=optimizer,\n","                loss_fn=loss_fn,\n","                epochs=5,\n","                device=device,\n","                writer=writer)"],"metadata":{"id":"HuR10Qn-zi4h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Tensorboard Results"],"metadata":{"id":"8hfftWFT3INS"}},{"cell_type":"code","source":["%load_ext tensorboard\n","%tensorboard --logdir runs"],"metadata":{"id":"YKGhuRPI2eIR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# SummaryWriter Function"],"metadata":{"id":"-DRro6EXMOke"}},{"cell_type":"code","source":["def create_writer(\n","    experiment_name: str, \n","    model_name: str, \n","    extra: str=None):\n","  \n","  from datetime import datetime\n","  import os\n","  timestamp = datetime.now().strftime(\"%Y-%m-%d\")\n","  if extra:\n","    log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name, extra)\n","  else:\n","    log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name)\n","  print(f\"[INFO] Created SummaryWriter, saving to: {log_dir}...\")\n","  return SummaryWriter(log_dir=log_dir)"],"metadata":{"id":"aF8JF0AM2eLK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create an example writer\n","example_writer = create_writer(experiment_name=\"data_10_percent\",\n","                               model_name=\"effnetb0\",\n","                               extra=\"5_epochs\")"],"metadata":{"id":"9I9-2e3_2eOA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model Experiment"],"metadata":{"id":"pT-hl_j1VbrP"}},{"cell_type":"code","source":["# Download 10 percent and 20 percent training data (if necessary)\n","data_10_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n","                                     destination=\"pizza_steak_sushi\")\n","\n","data_20_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\",\n","                                     destination=\"pizza_steak_sushi_20_percent\")"],"metadata":{"id":"7KQ-wNEh2eRD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Setup training directory paths\n","train_dir_10_percent = data_10_percent_path / \"train\"\n","train_dir_20_percent = data_20_percent_path / \"train\"\n","\n","# Setup testing directory paths (note: use the same test dataset for both to compare the results)\n","test_dir = data_10_percent_path / \"test\"\n","\n","# Check the directories\n","print(f\"Training directory 10%: {train_dir_10_percent}\")\n","print(f\"Training directory 20%: {train_dir_20_percent}\")\n","print(f\"Testing directory: {test_dir}\")"],"metadata":{"id":"ccaVPZjb2eTx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torchvision import transforms\n","\n","# Create a transform to normalize data distribution to be inline with ImageNet\n","normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], # values per colour channel [red, green, blue]\n","                                 std=[0.229, 0.224, 0.225]) # values per colour channel [red, green, blue]\n","\n","# Compose transforms into a pipeline\n","simple_transform = transforms.Compose([\n","    transforms.Resize((224, 224)), # 1. Resize the images\n","    transforms.ToTensor(), # 2. Turn the images into tensors with values between 0 & 1\n","    normalize # 3. Normalize the images so their distributions match the ImageNet dataset \n","])"],"metadata":{"id":"b8G-Mm7_2eW5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["BATCH_SIZE = 32\n","\n","# Create 10% training and test DataLoaders\n","train_dataloader_10_percent, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir_10_percent,\n","    test_dir=test_dir, \n","    transform=simple_transform,\n","    batch_size=BATCH_SIZE\n",")\n","\n","# Create 20% training and test data DataLoders\n","train_dataloader_20_percent, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir_20_percent,\n","    test_dir=test_dir,\n","    transform=simple_transform,\n","    batch_size=BATCH_SIZE\n",")\n","\n","# Find the number of samples/batches per dataloader (using the same test_dataloader for both experiments)\n","print(f\"Number of batches of size {BATCH_SIZE} in 10 percent training data: {len(train_dataloader_10_percent)}\")\n","print(f\"Number of batches of size {BATCH_SIZE} in 20 percent training data: {len(train_dataloader_20_percent)}\")\n","print(f\"Number of batches of size {BATCH_SIZE} in testing data: {len(train_dataloader_10_percent)} (all experiments will use the same test set)\")\n","print(f\"Number of classes: {len(class_names)}, class names: {class_names}\")"],"metadata":{"id":"sC5HoyOw2eZg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torchvision\n","from torchinfo import summary\n","\n","# 1. Create an instance of EffNetB2 with pretrained weights\n","effnetb2_weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT # \"DEFAULT\" means best available weights\n","effnetb2 = torchvision.models.efficientnet_b2(weights=effnetb2_weights)\n","\n","# # 2. Get a summary of standard EffNetB2 from torchvision.models (uncomment for full output)\n","# summary(model=effnetb2, \n","#         input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"\n","#         # col_names=[\"input_size\"], # uncomment for smaller output\n","#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n","#         col_width=20,\n","#         row_settings=[\"var_names\"]\n","# ) \n","\n","# 3. Get the number of in_features of the EfficientNetB2 classifier layer\n","print(f\"Number of in_features to final layer of EfficientNetB2: {len(effnetb2.classifier.state_dict()['1.weight'][0])}\")"],"metadata":{"id":"A4v9kOsuQtOp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torchvision\n","from torch import nn\n","\n","# Get num out features (one for each class pizza, steak, sushi)\n","OUT_FEATURES = len(class_names)\n","\n","# Create an EffNetB0 feature extractor\n","def create_effnetb0():\n","    # 1. Get the base mdoel with pretrained weights and send to target device\n","    weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n","    model = torchvision.models.efficientnet_b0(weights=weights).to(device)\n","\n","    # 2. Freeze the base model layers\n","    for param in model.features.parameters():\n","        param.requires_grad = False\n","\n","\n","\n","    # 4. Change the classifier head\n","    model.classifier = nn.Sequential(\n","        nn.Dropout(p=0.2),\n","        nn.Linear(in_features=1280, out_features=OUT_FEATURES)\n","    ).to(device)\n","\n","    # 5. Give the model a name\n","    model.name = \"effnetb0\"\n","    print(f\"[INFO] Created new {model.name} model.\")\n","    return model\n","\n","# Create an EffNetB2 feature extractor\n","def create_effnetb2():\n","    # 1. Get the base model with pretrained weights and send to target device\n","    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n","    model = torchvision.models.efficientnet_b2(weights=weights).to(device)\n","\n","    # 2. Freeze the base model layers\n","    for param in model.features.parameters():\n","        param.requires_grad = False\n","\n","    # 3. Set the seeds\n","\n","\n","    # 4. Change the classifier head\n","    model.classifier = nn.Sequential(\n","        nn.Dropout(p=0.3),\n","        nn.Linear(in_features=1408, out_features=OUT_FEATURES)\n","    ).to(device)\n","\n","    # 5. Give the model a name\n","    model.name = \"effnetb2\"\n","    print(f\"[INFO] Created new {model.name} model.\")\n","    return model"],"metadata":{"id":"5_dyBmqtQtRr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["effnetb0 = create_effnetb0() \n","\n","# Get an output summary of the layers in our EffNetB0 feature extractor model (uncomment to view full output)\n","summary(model=effnetb0, \n","        input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"\n","        # col_names=[\"input_size\"], # uncomment for smaller output\n","        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n","        col_width=20,\n","        row_settings=[\"var_names\"]\n",") "],"metadata":{"id":"H6VcGdV3QtUe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["effnetb2 = create_effnetb2()\n","\n","# Get an output summary of the layers in our EffNetB2 feature extractor model (uncomment to view full output)\n","summary(model=effnetb2, \n","        input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"\n","        # col_names=[\"input_size\"], # uncomment for smaller output\n","        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n","        col_width=20,\n","        row_settings=[\"var_names\"]\n",") "],"metadata":{"id":"3eQsYrQeQtXd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1. Create epochs list\n","num_epochs = [5, 10]\n","\n","# 2. Create models list (need to create a new model for each experiment)\n","models = [\"effnetb0\", \"effnetb2\"]\n","\n","# 3. Create dataloaders dictionary for various dataloaders\n","train_dataloaders = {\"data_10_percent\": train_dataloader_10_percent,\n","                     \"data_20_percent\": train_dataloader_20_percent}"],"metadata":{"id":"38AUiW7oQtaf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","from Basic_Modules.utils import save_model\n","\n","\n","# 2. Keep track of experiment numbers\n","experiment_number = 0\n","\n","# 3. Loop through each DataLoader\n","for dataloader_name, train_dataloader in train_dataloaders.items():\n","\n","    # 4. Loop through each number of epochs\n","    for epochs in num_epochs: \n","\n","        # 5. Loop through each model name and create a new model based on the name\n","        for model_name in models:\n","\n","            # 6. Create information print outs\n","            experiment_number += 1\n","            print(f\"[INFO] Experiment number: {experiment_number}\")\n","            print(f\"[INFO] Model: {model_name}\")\n","            print(f\"[INFO] DataLoader: {dataloader_name}\")\n","            print(f\"[INFO] Number of epochs: {epochs}\")  \n","\n","            # 7. Select the model\n","            if model_name == \"effnetb0\":\n","                model = create_effnetb0() # creates a new model each time (important because we want each experiment to start from scratch)\n","            else:\n","                model = create_effnetb2() # creates a new model each time (important because we want each experiment to start from scratch)\n","            \n","            # 8. Create a new loss and optimizer for every model\n","            loss_fn = nn.CrossEntropyLoss()\n","            optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n","\n","            # 9. Train target model with target dataloaders and track experiments\n","            train(model=model,\n","                  train_dataloader=train_dataloader,\n","                  test_dataloader=test_dataloader, \n","                  optimizer=optimizer,\n","                  loss_fn=loss_fn,\n","                  epochs=epochs,\n","                  device=device,\n","                  writer=create_writer(experiment_name=dataloader_name,\n","                                       model_name=model_name,\n","                                       extra=f\"{epochs}_epochs\"))\n","            \n","            # 10. Save the model to file so we can get back the best model\n","            save_filepath = f\"07_{model_name}_{dataloader_name}_{epochs}_epochs.pth\"\n","            save_model(model=model,\n","                       target_dir=\"models\",\n","                       model_name=save_filepath)\n","            print(\"-\"*50 + \"\\n\")"],"metadata":{"id":"QE_bq8z-RFXF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Viewing TensorBoard in Jupyter and Google Colab Notebooks (uncomment to view full TensorBoard instance)\n","%load_ext tensorboard\n","%tensorboard --logdir runs"],"metadata":{"id":"M8OXNizzRFaR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"6CY1OUU4RFdG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ZM5BThUGRFf3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"kJh7Wo55RFiv"},"execution_count":null,"outputs":[]}]}